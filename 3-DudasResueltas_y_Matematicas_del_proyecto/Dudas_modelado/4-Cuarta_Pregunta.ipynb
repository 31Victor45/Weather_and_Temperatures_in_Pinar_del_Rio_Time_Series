{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae3155a",
   "metadata": {},
   "source": [
    "# 游댕 Interdependencia y Modelado Vectorial: El rol de la Causalidad de Granger\n",
    "\n",
    "Los resultados que obtuviste son la \"partida de nacimiento\" t칠cnica de tu modelo. Entender la diferencia entre una relaci칩n unidireccional (M치xima -> M칤nima) y el funcionamiento interno de **VARMAX** es clave para defender tu proyecto. Aqu칤 tienes las respuestas detalladas para tu carpeta de dudas.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 쮺칩mo se le \"dice\" al modelo que una temperatura ayuda a la otra?\n",
    "En los modelos de Machine Learning cl치sicos, t칰 eliges manualmente las \"X\" (entradas) para predecir \"Y\" (salida). En **VARMAX (Vector Autoregressive Moving Average with eXogenous variables)**, el proceso es distinto:\n",
    "\n",
    "- **No hay una sola \"Y\":** Al pasarle al modelo un DataFrame con **dos columnas** (`Temp_Maxima` y `Temp_Minima`), le est치s diciendo impl칤citamente: *\"Estas dos variables pertenecen al mismo sistema\"*.\n",
    "- **Construcci칩n autom치tica:** El modelo construye internamente un sistema de ecuaciones donde **cada variable es, al mismo tiempo, causa y efecto**. No necesitas programar la relaci칩n; el algoritmo de VARMAX busca los pesos (coeficientes) que vinculan el pasado de la M치xima con el presente de la M칤nima autom치ticamente.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 쮽ue vital esta conclusi칩n para elegir VARIMA?\n",
    "**S칤, absolutamente.** Es una de las mejores pr치cticas en econometr칤a y meteorolog칤a estad칤stica. \n",
    "\n",
    "- Si el Test de Granger hubiera dicho que **ninguna** variable ayuda a la otra (p-valores > 0.05 en ambos sentidos), usar un modelo vectorial (VARMAX) ser칤a un error o un desperdicio de recursos. En ese caso, lo correcto habr칤a sido hacer dos modelos **ARIMA** independientes (uno para cada temperatura).\n",
    "- **Tu hallazgo [B] (p = 0.00000)** justifica cient칤ficamente por qu칠 no puedes separar las temperaturas. Al predecir la M칤nima, el modelo \"sabe\" que debe mirar qu칠 tan alto lleg칩 el sol (M치xima) horas antes.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 쮻e qu칠 manera afect칩 esta prueba a la construcci칩n?\n",
    "La prueba de Granger afect칩 el modelo en tres niveles:\n",
    "1. **Selecci칩n del Algoritmo:** Pasamos de pensar en \"univariante\" (un hilo) a \"multivariante\" (una red de hilos).\n",
    "2. **Interpretaci칩n de Errores:** Ahora sabemos que si el modelo falla en la M치xima, es muy probable que tambi칠n falle en la M칤nima debido a esa conexi칩n que probamos.\n",
    "3. **Validaci칩n de Datos:** Confirm칩 que el preprocesamiento (Diferenciaci칩n) no borr칩 la informaci칩n f칤sica que une a ambas variables.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 쮼l modelo se predice a s칤 mismo o a la otra variable?\n",
    "**La respuesta es: AMBAS cosas al mismo tiempo.** En VARMAX, cada predicci칩n es una suma de dos fuerzas:\n",
    "1. **Auto-regresi칩n (Inercia propia):** El modelo usa el pasado de la M치xima para predecir la M치xima.\n",
    "2. **Regresi칩n Cruzada (Influencia externa):** El modelo usa el pasado de la M치xima para predecir la M칤nima (y viceversa, aunque en tu caso el peso de la M칤nima hacia la M치xima sea casi cero seg칰n Granger).\n",
    "\n",
    "**Analog칤a del Baile:**\n",
    "Imagina a dos bailarines (M치xima y M칤nima).\n",
    "- Cada bailar칤n tiene su propio ritmo (**Auto-regresi칩n**).\n",
    "- Pero el bailar칤n A (M치xima) gu칤a los movimientos del bailar칤n B (**Granger Causality**).\n",
    "- El modelo **VARMAX** es el core칩grafo que observa a ambos y entiende que, aunque cada uno tiene sus pies, el movimiento de uno afecta la posici칩n del otro.\n",
    "\n",
    "---\n",
    "\n",
    "### 游눠 Resumen T칠cnico:\n",
    "> \"La Causalidad de Granger no es un manual de instrucciones para el c칩digo, sino una **justificaci칩n cient칤fica**. Al pasar ambas columnas al objeto `VARMAX`, el modelo utiliza el resultado de Granger para asignar coeficientes altos a la relaci칩n **M치xima -> M칤nima** y coeficientes bajos (o nulos) a la relaci칩n **M칤nima -> M치xima**, optimizando as칤 la precisi칩n del sistema completo.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
